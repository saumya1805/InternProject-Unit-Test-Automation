package platform.monitoring.eventlogfile.insights;

import static platform.monitoring.eventlogfile.insights.AdminAnalyticsDAO.NO_LIMIT;
import static platform.monitoring.eventlogfile.insights.EventLogFileAnalyticsMessageQueueParams.PARAM_EVENT_LOG_FILE_ANALYTIC_JOB_ID;
import static platform.monitoring.eventlogfile.insights.EventLogFileAnalyticsMessageQueueParams.PARAM_ORGANIZATION_ID;
import static platform.monitoring.eventlogfile.insights.EventLogFileAnalyticsMessageQueueParams.PARAM_RELATED_REQUEST_ID;
import java.sql.SQLException;
import java.util.Calendar;
import java.util.Map;
import java.util.concurrent.TimeUnit;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Lazy;
import org.springframework.stereotype.Service;
import com.force.commons.util.stack.StackUtils;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Stopwatch;
import common.api.soap.Entity;
import platform.monitoring.eventlogfile.logging.EventLogFileInsightsIntegration.Status;
import platform.monitoring.eventlogfile.udd.EventLogFileAnalyticJobDAO;
import platform.monitoring.eventlogfile.udd.EventLogFileAnalyticJobObject;
import platform.monitoring.eventlogfile.udd.EventLogFileObject;
import platform.monitoring.eventlogfile.udd.EventLogFileUploadStatus;

@Service
@Lazy
public Abstract class EventLogFileAppendUploader {

        @Autowired
        protected EventLogFileAnalyticsLogger logger;

        @Autowired
        public EventLogFileAnalyticsLogMessageGenerator messageGenerator;

        @Autowired
        private EventLogFileAnalyticsPermChecker permChecker;

        @Autowired
        private EventLogFileAnalyticJobDAO elfAnalyticJobDAO;

        @Autowired
        private EventLogFileAnalyticsAutoProcUserDequeueContextHandler autoProcDequeueContextHandler;

        @Autowired
        private EventLogFileAnalyticsDateDeterminer dateDeterminer;

        @Autowired
        private EventLogFileAnalyticsDatabaseHandler databaseHandler;

      public void processMessage(Map<String, String> jobParamMap) throws Exception {

            final String relatedRequestId = jobParamMap.get(PARAM_RELATED_REQUEST_ID);
            final String elfAnalyticsJobId = jobParamMap.get(PARAM_EVENT_LOG_FILE_ANALYTIC_JOB_ID);
            final String orgId = jobParamMap.get(PARAM_ORGANIZATION_ID);

            autoProcDequeueContextHandler.establishContextAsAutoProcUser(orgId, relatedRequestId);
            EventLogFileAnalyticJobObject jobObject = getEventLogFileAnalyticObject(elfAnalyticsJobId, relatedRequestId, orgId);

            AnalyticsAppEnum analyticsAppEnum = AnalyticsAppEnum.fromTemplateName(jobObject.getTemplateSource().getAppTemplateName());
            EventLogFileFrequency eventLogFileFrequency = EventLogFileFrequency.fromEventLogIntervalEnumApiValue(jobObject.getEventLogFileInterval().getApiValue());

            final Integer storagePeriod = jobObject.getStoragePeriod();
            final String eventTypeApiValue = jobObject.getEventType().getApiValue();
            final String folderId = jobObject.getFolder();
            final String datasetName = jobObject.getDatasetName();
            Stopwatch stopwatch = Stopwatch.createStarted();
            try {
                // make sure that permissions are still there required for the integration
                boolean isRequiredOrgBitsEnabled = permChecker.checkRequiredOrgBits(orgId, eventTypeApiValue, folderId, eventLogFileFrequency, analyticsAppEnum);

                // do processing only if the org perms are enabled for this org.
                 EventLogFileUploadStatus status = null;
                 String insightsExternalDataId = null;
                 long totalETLFileSizeInBytes = -1;

                 if (isRequiredOrgBitsEnabled) {
                     final Entity[] elfEntities = getEntities(jobObject, orgId);
                     String logMsg = messageGenerator.getElfStatusMsg(elfEntities.length, storagePeriod);
                     logger.writeLog(eventLogFileFrequency, Status.SUCCESS, orgId, eventTypeApiValue, folderId, logMsg);

                     if (elfEntities.length > 0) {
                         AdminAnalyticsETLProcess etlProcess = createETLProcessInstance(eventLogFileFrequency);

                         // perform the extract transform and load process
                         etlProcess.performETL(eventTypeApiValue, folderId, datasetName, elfEntities);

                         // set the state of the event log file analytic job to in progress, to denote wave infra ingest is in progress.
                         status = EventLogFileUploadStatus.UPLOAD_IN_PROGRESS;
                         insightsExternalDataId = etlProcess.getInsightsExternalDataId();
                         totalETLFileSizeInBytes = etlProcess.getTotalETLFileSizeInBytes();
                     } else {
                         status = EventLogFileUploadStatus.UPLOAD_SKIPPED;
                     }
                 } else {
                     status = EventLogFileUploadStatus.UPLOAD_FAILED;
                 }

                 elfAnalyticJobDAO.updateStatusAndInsightsExternalDataId(jobObject, status, insightsExternalDataId);
              commit();

                 String successfullyStatusSetMsg = (status == EventLogFileUploadStatus.UPLOAD_IN_PROGRESS) ?
                         messageGenerator.getCompletedStateMachineMsg() :
                         messageGenerator.getSuccessfullySetElfAnalyticsJobStatusMsg(status.getApiValue());

                 long elapsedTimeMillis = stopwatch.elapsed(TimeUnit.MILLISECONDS);
                 logger.writeLog(eventLogFileFrequency, Status.SUCCESS, orgId, eventTypeApiValue, folderId, elapsedTimeMillis, totalETLFileSizeInBytes, relatedRequestId, successfullyStatusSetMsg);

             } catch (Exception ex) {
                 final String errorMessage = messageGenerator.getDequeueFailureMsg(datasetName, String.valueOf(storagePeriod), StackUtils.getStackTrace(ex));
                 long elapsedTimeMillis = stopwatch.elapsed(TimeUnit.MILLISECONDS);
                 logger.writeLog(eventLogFileFrequency, Status.FAILURE, orgId, eventTypeApiValue, folderId, elapsedTimeMillis, relatedRequestId, errorMessage);

                 /**
                  * NOTE: Please do not set the status of the eflAnalyticJobObject to failed here. Throw the error and let
                  * {@link AdminAnalyticsMessageHandler} set the status after the retries are exhausted
                  */
                 throw ex;
             }
         }


         @VisibleForTesting
         public Entity[] getEntities(EventLogFileAnalyticJobObject elfAnalyticJobObject, String orgId) throws Exception {
             EventLogFileFrequency eventLogFileFrequency = EventLogFileFrequency
                     .fromEventLogIntervalEnumApiValue(elfAnalyticJobObject.getEventLogFileInterval().getApiValue());

             // calculate the min log date from the jobs upload end date and storage period
             Calendar minLogDate = dateDeterminer.getDaysAgo(elfAnalyticJobObject.getUploadEndDate(), elfAnalyticJobObject.getStoragePeriod());

             // we'll have to truncate the calculated min log date to set hour, min, and sec to zero, because it'll be used to compare against ELF.LogDate
             Calendar minLogDateTruncated =  dateDeterminer.truncateToMidnight(minLogDate);

             Entity[] elfEntities = eventLogFileFrequency.getAdminAnalyticsDao().getEventLogFilesBetweenStartAndEndDate(
                     elfAnalyticJobObject.getEventType().getApiValue(), NO_LIMIT, orgId,
                     elfAnalyticJobObject.getUploadStartDate(), elfAnalyticJobObject.getUploadEndDate(), minLogDateTruncated);
             if (databaseHandler.shouldCommit()) { databaseHandler.releaseDBConnection(); }
            return elfEntities;
         }

         EventLogFileAnalyticJobObject getEventLogFileAnalyticObject(String elfAnalyticsJobId,
              String relatedRequestId, String orgId) throws SQLException {
             EventLogFileAnalyticJobObject elfAnalyticJobObject;
             try {
                 elfAnalyticJobObject = elfAnalyticJobDAO.load(elfAnalyticsJobId);
             } catch (SQLException ex) {
                 String errorMessage = messageGenerator.getEventLogFileAnalyticsJobFailureMsg("load",
                         StackUtils.getStackTrace(ex));

                 logger.writeLog(Status.FAILURE, orgId, relatedRequestId, errorMessage);
                 throw ex;
             }
             return elfAnalyticJobObject;
         }
     }